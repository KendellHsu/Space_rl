{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# DDQN model\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.join(os.getcwd(), 'space_ship_game_RL')\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "from setting import WIDTH, HEIGHT\n",
    "from game import Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceShipEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gymnasium version of the SpaceShip environment.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self, frame_stack: int = 4, render_mode: str = None):\n",
    "        super().__init__()\n",
    "        self.frame_stack = frame_stack\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # 建立遊戲實例\n",
    "        self.game = Game(frame_stack=frame_stack)\n",
    "\n",
    "        # 動作空間\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # 觀測空間 shape=(H, W, 3*frame_stack)\n",
    "        sample = self.game.stacked_state  # shape=(W, H, C)\n",
    "        h, w, c = np.transpose(sample, (1,0,2)).shape\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(h, w, c),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # 延遲初始化 Pygame 視窗\n",
    "        self.window = None\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Gymnasium 正確的 reset signature\n",
    "        super().reset(seed=seed)\n",
    "        # 重建遊戲，清空 deque\n",
    "        self.game = Game(frame_stack=self.frame_stack)\n",
    "\n",
    "        obs = np.transpose(self.game.stacked_state, (1,0,2))\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    # 在 spaceship_env 或 spaceship_env_gymnasium 裡的 step()\n",
    "    def step(self, action):\n",
    "        # 1) 先記錄前一刻的關鍵數值\n",
    "        prev_score  = self.game.score\n",
    "        prev_health = self.game.player.sprite.health\n",
    "        prev_gun     = self.game.player.sprite.gun\n",
    "\n",
    "        # 2) 執行動作與遊戲邏輯\n",
    "        self.game.update(action)\n",
    "\n",
    "        # 3) 蒐集當前數值\n",
    "        cur_score  = self.game.score\n",
    "        cur_health = self.game.player.sprite.health\n",
    "        cur_gun     = self.game.player.sprite.gun\n",
    "\n",
    "        # 4) 基本打中隕石的分數增量\n",
    "        reward  = float(cur_score - prev_score)\n",
    "\n",
    "        # 5) 存活時間獎勵：每步 +0.1\n",
    "        reward += 0.1\n",
    "\n",
    "        # 6) 被石頭打到的懲罰：依受傷量負向獎勵\n",
    "        damage = prev_health - cur_health\n",
    "        if damage > 0:\n",
    "            # 例如：每 1 點血量扣 0.5 分\n",
    "            reward -= 1 * damage\n",
    "\n",
    "        # 7) 吃到回血道具的獎勵：依回血量正向獎勵\n",
    "        heal = cur_health - prev_health\n",
    "        if heal > 0:\n",
    "            # 例如：每回血 1 點 +0.2 分\n",
    "            reward += 2 * heal\n",
    "\n",
    "        # 9) 判斷是否結束\n",
    "        terminated = (not self.game.running) or (cur_score >= 10000)\n",
    "        truncated  = False\n",
    "        info       = {}\n",
    "\n",
    "        # 10) 回傳 obs, reward, done\n",
    "        obs = np.transpose(self.game.stacked_state, (1,0,2))\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "            pygame.display.set_caption(\"SpaceShip RL (Gymnasium)\")\n",
    "        # 繪製當前一幀\n",
    "        self.game.draw(self.window)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.quit()\n",
    "            self.window = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 影像前處理\n",
    "def preprocess_stack(raw: np.ndarray, frame_stack: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    raw: np.ndarray, shape=(W, H, 3*frame_stack), dtype=uint8\n",
    "    return: torch.Tensor, shape=(frame_stack, H, W), float32 in [0,1]\n",
    "    \"\"\"\n",
    "    W, H, Ctot = raw.shape\n",
    "    C = Ctot // frame_stack\n",
    "    # 重塑並轉軸： (W, H, stack, C) → (stack, H, W, C)\n",
    "    frames = raw.reshape(W, H, frame_stack, C).transpose(2, 1, 0, 3)\n",
    "    # 轉灰階 + resize\n",
    "    gray_resized = []\n",
    "    for f in frames:\n",
    "        # 1. RGB→灰階\n",
    "        g = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY)\n",
    "        # 2. Resize 到 84×84\n",
    "        g84 = cv2.resize(g, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        gray_resized.append(g84)\n",
    "\n",
    "    stack = np.stack(gray_resized, axis=0)          # (stack, 84, 84)\n",
    "    tensor = torch.from_numpy(stack).float() / 255.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 網路結構 (灰階單通道)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_dim, frame_stack):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(frame_stack, 32, kernel_size=8, stride=4),  # 現在有 4 幀\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 經驗回放\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s_, d):\n",
    "        self.buffer.append((s, a, r, s_, d))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s_, d = map(np.array, zip(*batch))\n",
    "        return (torch.FloatTensor(s),\n",
    "                torch.LongTensor(a),\n",
    "                torch.FloatTensor(r),\n",
    "                torch.FloatTensor(s_),\n",
    "                torch.FloatTensor(d))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity: int = 10000, alpha: float = 0.6):\n",
    "        \"\"\"\n",
    "        capacity: 最大儲存筆數\n",
    "        alpha: priority 調整強度（0 = 均勻抽樣, 1 = 完全按 priority）\n",
    "        \"\"\"\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []                                   # list of (s,a,r,s_,done)\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.pos        = 0\n",
    "        self.alpha      = alpha\n",
    "\n",
    "    def push(self, s, a, r, s_, d):\n",
    "        \"\"\"加入一筆新的 transition，priority 給當前最大值保證先被抽到\"\"\"\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((s, a, r, s_, d))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (s, a, r, s_, d)\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int, beta: float = 0.4):\n",
    "        \"\"\"\n",
    "        按 priority 抽樣\n",
    "        beta: importance-sampling 校正強度 (0 = 無校正, 1 = 完全校正)\n",
    "        回傳 (s, a, r, s_, d, weights, indices)\n",
    "        \"\"\"\n",
    "        # 取出有效的 priorities\n",
    "        prios = self.priorities if len(self.buffer) == self.capacity else self.priorities[:len(self.buffer)]\n",
    "        # 計算抽樣機率\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        # 按概率抽 batch\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # importance-sampling weights\n",
    "        total   = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 拆包\n",
    "        s, a, r, s_, d = zip(*samples)\n",
    "        return (\n",
    "            torch.FloatTensor(s),\n",
    "            torch.LongTensor(a),\n",
    "            torch.FloatTensor(r),\n",
    "            torch.FloatTensor(s_),\n",
    "            torch.FloatTensor(d),\n",
    "            torch.FloatTensor(weights),\n",
    "            indices\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, losses, offset: float = 1e-6):\n",
    "        \"\"\"\n",
    "        用新的 TD-error (losses) 更新相應 indices 的 priorities\n",
    "        offset 避免 priority 變 0\n",
    "        \"\"\"\n",
    "        for idx, loss in zip(indices, losses):\n",
    "            self.priorities[idx] = abs(loss.item()) + offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "EPISODES       = 1000\n",
    "BATCH_SIZE     = 1024\n",
    "GAMMA          = 0.99\n",
    "EPS_START      = 1.0\n",
    "EPS_END        = 0.01\n",
    "EPS_DECAY      = 0.99\n",
    "LR             = 1e-4\n",
    "TARGET_UPDATE  = 5\n",
    "FRAME_STACK    = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27ca6385a00>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "env = SpaceShipEnv(frame_stack=FRAME_STACK, render_mode=None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "action_dim = env.action_space.n\n",
    "policy_net = DQN(action_dim, FRAME_STACK).to(device)\n",
    "target_net = DQN(action_dim, FRAME_STACK).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "buffer = ReplayBuffer(capacity=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   5 | Reward:   416.6 | Best:   416.6 | Epsilon: 0.956\n",
      "Episode  10 | Reward:    60.8 | Best:   416.6 | Epsilon: 0.914\n",
      "Episode  15 | Reward:    16.1 | Best:   423.9 | Epsilon: 0.873\n",
      "Episode  20 | Reward:   109.6 | Best:   429.1 | Epsilon: 0.835\n",
      "Episode  25 | Reward:    76.6 | Best:   511.1 | Epsilon: 0.798\n",
      "Episode  30 | Reward:   226.9 | Best:   542.9 | Epsilon: 0.762\n",
      "Episode  35 | Reward:   370.1 | Best:   542.9 | Epsilon: 0.729\n",
      "Episode  40 | Reward:   142.5 | Best:   542.9 | Epsilon: 0.697\n",
      "Episode  45 | Reward:   390.8 | Best:   777.5 | Epsilon: 0.666\n",
      "Episode  50 | Reward:   105.2 | Best:   777.5 | Epsilon: 0.636\n",
      "Episode  55 | Reward:   186.0 | Best:   777.5 | Epsilon: 0.608\n",
      "Episode  60 | Reward:   107.3 | Best:   777.5 | Epsilon: 0.581\n",
      "Episode  65 | Reward:   490.4 | Best:   777.5 | Epsilon: 0.556\n",
      "Episode  70 | Reward:   232.8 | Best:   777.5 | Epsilon: 0.531\n",
      "Episode  75 | Reward:   226.3 | Best:   782.9 | Epsilon: 0.508\n",
      "Episode  80 | Reward:    19.7 | Best:   924.8 | Epsilon: 0.485\n",
      "Episode  85 | Reward:  1107.9 | Best:  1107.9 | Epsilon: 0.464\n",
      "Episode  90 | Reward:   184.2 | Best:  1107.9 | Epsilon: 0.443\n",
      "Episode  95 | Reward:    26.6 | Best:  1107.9 | Epsilon: 0.424\n",
      "Episode 100 | Reward:   169.9 | Best:  1107.9 | Epsilon: 0.405\n",
      "Episode 105 | Reward:    75.8 | Best:  1107.9 | Epsilon: 0.387\n",
      "Episode 110 | Reward:   163.5 | Best:  1107.9 | Epsilon: 0.370\n",
      "Episode 115 | Reward:   776.6 | Best:  1107.9 | Epsilon: 0.354\n",
      "Episode 120 | Reward:   283.0 | Best:  1107.9 | Epsilon: 0.338\n",
      "Episode 125 | Reward:   158.1 | Best:  1107.9 | Epsilon: 0.323\n",
      "Episode 130 | Reward:    -6.3 | Best:  1107.9 | Epsilon: 0.309\n",
      "Episode 135 | Reward:   326.1 | Best:  1107.9 | Epsilon: 0.295\n",
      "Episode 140 | Reward:    72.7 | Best:  1107.9 | Epsilon: 0.282\n",
      "Episode 145 | Reward:   277.5 | Best:  1107.9 | Epsilon: 0.270\n",
      "Episode 150 | Reward:    -6.1 | Best:  1107.9 | Epsilon: 0.258\n",
      "Episode 155 | Reward:   -58.3 | Best:  1107.9 | Epsilon: 0.246\n",
      "Episode 160 | Reward:   371.8 | Best:  1107.9 | Epsilon: 0.235\n",
      "Episode 165 | Reward:   227.7 | Best:  1411.2 | Epsilon: 0.225\n",
      "Episode 170 | Reward:    57.9 | Best:  1411.2 | Epsilon: 0.215\n",
      "Episode 175 | Reward:    11.9 | Best:  1411.2 | Epsilon: 0.206\n",
      "Episode 180 | Reward:    46.3 | Best:  1411.2 | Epsilon: 0.196\n",
      "Episode 185 | Reward:   -86.5 | Best:  1411.2 | Epsilon: 0.188\n",
      "Episode 190 | Reward:   284.5 | Best:  1411.2 | Epsilon: 0.179\n",
      "Episode 195 | Reward:   112.3 | Best:  1411.2 | Epsilon: 0.172\n",
      "Episode 200 | Reward:   -42.0 | Best:  1411.2 | Epsilon: 0.164\n",
      "Episode 205 | Reward:   149.5 | Best:  1411.2 | Epsilon: 0.157\n",
      "Episode 210 | Reward:   217.6 | Best:  1411.2 | Epsilon: 0.150\n",
      "Episode 215 | Reward:  1148.3 | Best:  1411.2 | Epsilon: 0.143\n",
      "Episode 220 | Reward:   819.1 | Best:  1411.2 | Epsilon: 0.137\n",
      "Episode 225 | Reward:   376.2 | Best:  1411.2 | Epsilon: 0.131\n",
      "Episode 230 | Reward:   -79.6 | Best:  1411.2 | Epsilon: 0.125\n",
      "Episode 235 | Reward:   187.6 | Best:  1411.2 | Epsilon: 0.119\n",
      "Episode 240 | Reward:   -54.6 | Best:  1411.2 | Epsilon: 0.114\n",
      "Episode 245 | Reward:    90.6 | Best:  1411.2 | Epsilon: 0.109\n",
      "Episode 250 | Reward:   325.2 | Best:  1411.2 | Epsilon: 0.104\n",
      "Episode 255 | Reward:   -31.1 | Best:  1411.2 | Epsilon: 0.100\n",
      "Episode 260 | Reward:   411.0 | Best:  1411.2 | Epsilon: 0.095\n",
      "Episode 265 | Reward:   266.1 | Best:  1411.2 | Epsilon: 0.091\n",
      "Episode 270 | Reward:   -69.5 | Best:  1411.2 | Epsilon: 0.087\n",
      "Episode 275 | Reward:   402.2 | Best:  1411.2 | Epsilon: 0.083\n",
      "Episode 280 | Reward:   -34.1 | Best:  1411.2 | Epsilon: 0.080\n",
      "Episode 285 | Reward:   153.0 | Best:  1411.2 | Epsilon: 0.076\n",
      "Episode 290 | Reward:   365.5 | Best:  1411.2 | Epsilon: 0.073\n",
      "Episode 295 | Reward:   180.3 | Best:  1411.2 | Epsilon: 0.069\n",
      "Episode 300 | Reward:   663.3 | Best:  1411.2 | Epsilon: 0.066\n",
      "Episode 305 | Reward:   229.2 | Best:  1411.2 | Epsilon: 0.063\n",
      "Episode 310 | Reward:   -27.5 | Best:  1411.2 | Epsilon: 0.061\n",
      "Episode 315 | Reward:   117.1 | Best:  1411.2 | Epsilon: 0.058\n",
      "Episode 320 | Reward:   169.0 | Best:  1411.2 | Epsilon: 0.055\n",
      "Episode 325 | Reward:   219.4 | Best:  1411.2 | Epsilon: 0.053\n",
      "Episode 330 | Reward:    92.6 | Best:  1411.2 | Epsilon: 0.051\n",
      "Episode 335 | Reward:   333.8 | Best:  1411.2 | Epsilon: 0.048\n",
      "Episode 340 | Reward:   365.8 | Best:  1411.2 | Epsilon: 0.046\n",
      "Episode 345 | Reward:    33.8 | Best:  1411.2 | Epsilon: 0.044\n",
      "Episode 350 | Reward:   -37.2 | Best:  1411.2 | Epsilon: 0.042\n",
      "Episode 355 | Reward:   477.0 | Best:  1411.2 | Epsilon: 0.040\n",
      "Episode 360 | Reward:   -33.5 | Best:  1411.2 | Epsilon: 0.039\n",
      "Episode 365 | Reward:   204.5 | Best:  1411.2 | Epsilon: 0.037\n",
      "Episode 370 | Reward:    37.0 | Best:  1411.2 | Epsilon: 0.035\n",
      "Episode 375 | Reward:    66.7 | Best:  1411.2 | Epsilon: 0.034\n",
      "Episode 380 | Reward:   244.1 | Best:  1411.2 | Epsilon: 0.032\n",
      "Episode 385 | Reward:  -129.0 | Best:  1411.2 | Epsilon: 0.031\n",
      "Episode 390 | Reward:   126.3 | Best:  1411.2 | Epsilon: 0.029\n",
      "Episode 395 | Reward:   432.8 | Best:  1411.2 | Epsilon: 0.028\n",
      "Episode 400 | Reward:   339.6 | Best:  1411.2 | Epsilon: 0.027\n",
      "Episode 405 | Reward:   616.5 | Best:  1411.2 | Epsilon: 0.026\n",
      "Episode 410 | Reward:   244.8 | Best:  1411.2 | Epsilon: 0.025\n",
      "Episode 415 | Reward:    29.9 | Best:  1411.2 | Epsilon: 0.023\n",
      "Episode 420 | Reward:  1478.7 | Best:  1478.7 | Epsilon: 0.022\n",
      "Episode 425 | Reward:   429.6 | Best:  1478.7 | Epsilon: 0.021\n",
      "Episode 430 | Reward:   195.1 | Best:  1478.7 | Epsilon: 0.020\n",
      "Episode 435 | Reward:   972.9 | Best:  1478.7 | Epsilon: 0.020\n",
      "Episode 440 | Reward:   937.2 | Best:  1478.7 | Epsilon: 0.019\n",
      "Episode 445 | Reward:   136.0 | Best:  1478.7 | Epsilon: 0.018\n",
      "Episode 450 | Reward:   652.1 | Best:  1478.7 | Epsilon: 0.017\n",
      "Episode 455 | Reward:   937.7 | Best:  1478.7 | Epsilon: 0.016\n",
      "Episode 460 | Reward:   770.5 | Best:  1478.7 | Epsilon: 0.016\n",
      "Episode 465 | Reward:   497.8 | Best:  1478.7 | Epsilon: 0.015\n",
      "Episode 470 | Reward:   465.7 | Best:  1478.7 | Epsilon: 0.014\n",
      "Episode 475 | Reward:  1089.4 | Best:  1478.7 | Epsilon: 0.014\n",
      "Episode 480 | Reward:   610.3 | Best:  1478.7 | Epsilon: 0.013\n",
      "Episode 485 | Reward:   403.9 | Best:  1478.7 | Epsilon: 0.012\n",
      "Episode 490 | Reward:   633.9 | Best:  1478.7 | Epsilon: 0.012\n",
      "Episode 495 | Reward:   178.0 | Best:  1478.7 | Epsilon: 0.011\n",
      "Episode 500 | Reward:   248.2 | Best:  1478.7 | Epsilon: 0.011\n"
     ]
    }
   ],
   "source": [
    "# 主訓練迴圈\n",
    "epsilon = EPS_START\n",
    "reward_history = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(1, EPISODES+1):\n",
    "    # reset → obs, info\n",
    "    raw, _ = env.reset()\n",
    "    state = preprocess_stack(raw, FRAME_STACK).to(device)\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedy 取動作\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(1).item()\n",
    "\n",
    "        # step → obs, reward, terminated, truncated, info\n",
    "        raw_next, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = preprocess_stack(raw_next, FRAME_STACK).to(device)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 存入回放記憶\n",
    "        buffer.push(state.cpu().numpy(),\n",
    "                    action,\n",
    "                    reward,\n",
    "                    next_state.cpu().numpy(),\n",
    "                    done)\n",
    "        state = next_state\n",
    "\n",
    "        # 4) 更新 policy_net\n",
    "        if len(buffer) >= BATCH_SIZE:\n",
    "            s, a, r, s_, d = buffer.sample(BATCH_SIZE)\n",
    "            s  = s.to(device)\n",
    "            a  = a.unsqueeze(1).to(device)\n",
    "            r  = r.unsqueeze(1).to(device)\n",
    "            s_ = s_.to(device)\n",
    "            d  = d.unsqueeze(1).to(device)\n",
    "\n",
    "            # 預測 Q(s,a)\n",
    "            q_pred = policy_net(s).gather(1, a)\n",
    "            # 計算 target Q\n",
    "            with torch.no_grad():\n",
    "                # 1) 用 policy_net 找到下一步最優動作 index\n",
    "                best_next_actions = policy_net(s_).argmax(dim=1, keepdim=True)\n",
    "                # 2) 用 target_net 評估這些動作的 Q 值\n",
    "                q_next = target_net(s_).gather(1, best_next_actions)\n",
    "                # 3) 組成 DDQN target\n",
    "                q_target = r + GAMMA * q_next * (1 - d)\n",
    "\n",
    "            loss = F.mse_loss(q_pred, q_target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 5) 更新 target network\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # 6) 紀錄 & 儲存最佳模型\n",
    "    reward_history.append(total_reward)\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        torch.save(policy_net.state_dict(), \"best_ddqn_space_ship.pth\")\n",
    "\n",
    "    # 7) 調整 ε\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "    # 8) 每 5 集回報一次\n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode:3d} | Reward: {total_reward:7.1f} | \"\n",
    "                f\"Best: {best_reward:7.1f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list=[]\n",
    "epsilon=EPS_START\n",
    "for i in range(500):\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "    epsilon_list.append(epsilon)\n",
    "plt.plot(epsilon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 畫出 reward 曲線圖\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plt.plot(\u001b[43mreward_history\u001b[49m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpisode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mTotal Reward\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'reward_history' is not defined"
     ]
    }
   ],
   "source": [
    "# 畫出 reward 曲線圖\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Reward Curve\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_trained_model(\n",
    "    model_path: str = \"best_dqn_space_ship.pth\",\n",
    "    frame_stack: int = 4,\n",
    "    device: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    以 human 模式播放一集遊戲，並印出總獎勵。\n",
    "    \"\"\"\n",
    "    # 選 GPU 或 CPU\n",
    "    device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # 1) 建立 Gymnasium 版環境（human 模式會自動 render）\n",
    "    env = SpaceShipEnv(frame_stack=frame_stack, render_mode=\"human\")\n",
    "\n",
    "    # 2) 建立並載入模型\n",
    "    action_dim = env.action_space.n\n",
    "    policy_net = DQN(action_dim, frame_stack).to(device)\n",
    "    checkpoint  = torch.load(model_path, map_location=device)\n",
    "    policy_net.load_state_dict(checkpoint)\n",
    "    policy_net.eval()\n",
    "\n",
    "    # 3) 重置環境\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # 4) 預處理第一個 state\n",
    "    state = preprocess_stack(obs, frame_stack).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    # 5) 播放並讓 agent 決策\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(state)        # shape (1, action_dim)\n",
    "            action   = q_values.argmax(1).item()\n",
    "\n",
    "        # step 並 render（human 模式 env.step 內會自動呼叫 render()）\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # 下一個 state\n",
    "        state = preprocess_stack(obs, frame_stack).unsqueeze(0).to(device)\n",
    "\n",
    "    # 6) 關閉 env 並印結果\n",
    "    env.close()\n",
    "    print(f\"▶️  Total Reward: {total_reward:.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_episode_as_video(env, video_path=\"gameplay.mp4\"):\n",
    "    frames = []\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 把畫面抓下來（RGB）\n",
    "        surface = pygame.display.get_surface()\n",
    "        frame = pygame.surfarray.array3d(surface)  # shape: (W, H, 3)\n",
    "        frame = np.transpose(frame, (1, 0, 2))     # pygame 是 x,y → imageio 是 y,x\n",
    "        frames.append(frame)\n",
    "\n",
    "\n",
    "    # 儲存成影片\n",
    "    imageio.mimsave(video_path, frames, fps=60, quality=9)\n",
    "    print(f\"Saved gameplay video to: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SpaceShipEnv()\n",
    "# record_episode_as_video(env, \"space_ship_run.mp4\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️  Total Reward: 1436.1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "play_with_trained_model(\n",
    "    model_path=\"best_ddqn_space_ship.pth\",\n",
    "    frame_stack=4,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "space_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
